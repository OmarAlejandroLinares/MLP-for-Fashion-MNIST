{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA0FvFe966r9"
   },
   "source": [
    "# Tarea 1_ Red neuronal Perceptrón multicapa Keras\n",
    "\n",
    "Este trabajo es un clasificador de prendas de ropa en 10 clases.\n",
    "Los resultados obtenidos fueron de:\n",
    "* Exactitud en el entrenamiento: 0.9332000017166138 \n",
    "* Pérdida en el entrenamiento: 0.17810234427452087\n",
    "* Exactitud en la prueba: 0.8931000232696533 \n",
    "* Pérdida en la prueba: 0.33242884278297424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nP5zZjkD7sWw"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aoy3nHOH3aDh"
   },
   "source": [
    "Esta tarea usa el set de datos de Fashion MNIST de más de 70,000 imágenes en 10 categorias. Las imágenes muestran artículos individuales de ropa a una resolucion de 28 por 28 pixeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cl5FegN27wzO"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "fashion_mnist = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq-Bj_Kar14P"
   },
   "source": [
    "# Separando el conjunto de datos en entrenamiento y prueba "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVqdN0_44TW2"
   },
   "source": [
    "Primero se separan los datos en conjuntos de entrenamiento y prueba con sus respectivas etiquetas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMlq0XNTwA6t"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test)= fashion_mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9nr7LYdO4c9G"
   },
   "source": [
    "Observamos que son 60k imágenes para el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dCFFRiSpwUdg",
    "outputId": "7d32f9a9-cba4-41fb-953c-8ba38aeab50a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1HTJRjUO4hUd"
   },
   "source": [
    "Se grafica una de estas imágenes para corroborar que se trate de un artículo de ropa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fpr0-pBNwWv_"
   },
   "outputs": [],
   "source": [
    "imagendemo=x_train[0,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "OkOVIgvnr14g",
    "outputId": "1005ee45-7573-42fc-a4ff-5b3e8e4e592b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1af74db390>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(imagendemo,cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFqV5anN4m2U"
   },
   "source": [
    "Posteriormente se estandarizan los valores de las imágenes a un rango de 0 a 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pMv3JSGt67Jg"
   },
   "source": [
    "Nota 1: Estandarizando las imágenes, mejora el porcentaje de accuracy del 0.8485 al 0.9395 con 15 épocas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T7_TM94AyQKl"
   },
   "outputs": [],
   "source": [
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwRo-mx_r14l"
   },
   "source": [
    "Se presentan las clases correspondientes y se almacenan en el diccionario label_dict{ }."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypNFROm8r14m"
   },
   "outputs": [],
   "source": [
    "label_dict = {\n",
    " 0: \"T-shirt/top\",\n",
    " 1: \"Trouser\",\n",
    " 2: \"Pullover\",\n",
    " 3: \"Dress\",\n",
    " 4: \"Coat\",\n",
    " 5: \"Sandal\",\n",
    " 6: \"Shirt\",\n",
    " 7: \"Sneaker\",\n",
    " 8: \"Bag\",\n",
    " 9: \"Ankle boot\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "edjTF1COr14q",
    "outputId": "896c4fe4-7cb0-42c2-f316-cea61c25341e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'Ankle boot'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict[y_train[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xt5gmRlGr14w"
   },
   "source": [
    "# ¡¡¡Suerte :D !!!\n",
    "\n",
    "Dankeschön!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbF-eq3LgRXo"
   },
   "source": [
    "## Ajustando los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OaGd-kYu547R"
   },
   "source": [
    "Se convierten las entradas bidimensionales a unidimensionales por medio del reshape, esto se podría realizar con una capa tipo \"Flatten\" pero desconocía si al agregar la capa ya no sería contemplada como una Red Vanilla jeje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D0BPN7W45vJJ",
    "outputId": "ea5edcfc-f724-4dfd-ecf2-cad60708c0be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 784)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train=x_train.reshape(-1,28*28).astype('float32')\n",
    "x_test=x_test.reshape(-1,28*28).astype('float32')\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6MrC1nVITxP"
   },
   "source": [
    "El convertir la salida de etiquetas a un vector one_hot cambia la exactitud de 89.11% a 89.16% con 5 épocas, por lo que se hace el cambio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uu1T9EacIQMP",
    "outputId": "4afd113d-7638-4ee8-9278-8fa64a0f660c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convertir el array de salidas a una codificación one_ hot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "# Cambiamos el y_train a una forma (numero de datos, etiqueta categorica)\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "# Y cambiamos a un vector one-hot\n",
    "y_train_onehot = onehot_encoder.fit_transform(y_train)\n",
    "\n",
    "# Cambiamos el y_test a una forma (numero de datos, etiqueta categorica)\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "# Y cambiamos a un vector one-hot\n",
    "y_test_onehot = onehot_encoder.fit_transform(y_test)\n",
    "\n",
    "y_train_onehot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ht4m_YUE3xC_"
   },
   "source": [
    "## Declarando la arquitectura con Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gBlya-gx6N8a"
   },
   "source": [
    "Ahora sí, la arquitectura de esta red son dos primeras capas densas de 128 neuronas y función de activación relu y una capa densa de salida de 10 neuronas puesto que queremos que clasifique entre 10 categorías distintas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9R6lK2856pU-"
   },
   "source": [
    "Nota 2: La función de activación de la última capa presentó mejoras al ser cambiada de 'None' (valor default si no es especificado) a 'softmax' (de 9.99% a 92.51%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJp1c2P_t_xW"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(128, input_shape=(784,), activation= 'relu'),\n",
    "    tf.keras.layers.Dense(128, activation= 'relu'),\n",
    "    tf.keras.layers.Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JruU-x8rLDzA",
    "outputId": "a34df1d0-c4e3-4bbf-b3f0-4020794f3432"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 128)               100480    \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 10)                1290      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118,282\n",
      "Trainable params: 118,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Graficar la arquitectura de la red\n",
    "model = model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTD8a2b8hLGi"
   },
   "source": [
    "## Entrenamiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVLmk0uy8VaG"
   },
   "source": [
    "Antes de que el modelo este listo para entrenar , se necesitan algunas configuraciones más. Éstas son agregadas durante el paso de compilacion del modelo:\n",
    "\n",
    "+ Loss function: Mide qué tan exacto es el modelo. Se busca minimizar su valor para dirigir el modelo en la dirección adecuada.\n",
    "+ Optimizer: Cómo el modelo se actualiza basado en el set de datos que ve y la función de pérdida. Por default el factor de aprendizaje es de 0.001\n",
    "+ Metrics: Métricas de validación, monitorean los pasos de entrenamiento y pruebas. El accuracy (exactitud), arroja la fración de entradas correctamente clasificadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvt4D57ouH8V"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h_1j4gFt9afl"
   },
   "source": [
    "Posteriormente se hace el entrenamiento para asociar las imágenes con sus respectivas etiquetas. Entre mayor sea la cantidad de épocas, mayor costo computacional requerirá pero en la mayoría de las veces el modelo se ajustará mejor a las entradas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ku9YHncSuKbz",
    "outputId": "6921105c-d96e-411b-c2ea-ae1fa250ffdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "600/600 [==============================] - 7s 10ms/step - loss: 0.5237 - accuracy: 0.8166\n",
      "Epoch 2/15\n",
      "600/600 [==============================] - 6s 9ms/step - loss: 0.3790 - accuracy: 0.8628\n",
      "Epoch 3/15\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.3331 - accuracy: 0.8788\n",
      "Epoch 4/15\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.3113 - accuracy: 0.8847\n",
      "Epoch 5/15\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.2936 - accuracy: 0.8919\n",
      "Epoch 6/15\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.2786 - accuracy: 0.8954\n",
      "Epoch 7/15\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.2665 - accuracy: 0.9003\n",
      "Epoch 8/15\n",
      "600/600 [==============================] - 3s 6ms/step - loss: 0.2575 - accuracy: 0.9045\n",
      "Epoch 9/15\n",
      "600/600 [==============================] - 4s 6ms/step - loss: 0.2435 - accuracy: 0.9089\n",
      "Epoch 10/15\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.2378 - accuracy: 0.9097\n",
      "Epoch 11/15\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.2300 - accuracy: 0.9138\n",
      "Epoch 12/15\n",
      "600/600 [==============================] - 4s 7ms/step - loss: 0.2232 - accuracy: 0.9157\n",
      "Epoch 13/15\n",
      "600/600 [==============================] - 5s 8ms/step - loss: 0.2122 - accuracy: 0.9195\n",
      "Epoch 14/15\n",
      "600/600 [==============================] - 4s 6ms/step - loss: 0.2052 - accuracy: 0.9219\n",
      "Epoch 15/15\n",
      "600/600 [==============================] - 4s 6ms/step - loss: 0.2008 - accuracy: 0.9240\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1af2a2e490>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y=y_train_onehot, batch_size =100, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vZcLW7Pw_HIq",
    "outputId": "73587e21-f713-45f7-d862-0c6a353b5813"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 - 3s - loss: 0.1781 - accuracy: 0.9332 - 3s/epoch - 1ms/step\n",
      "\n",
      "Exactitud en el entrenamiento: 0.9332000017166138 \n",
      "Pérdida en el entrenamiento: 0.17810234427452087\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc = model.evaluate(x_train, y_train_onehot, verbose=2)\n",
    "\n",
    "print('\\nExactitud en el entrenamiento:', train_acc, '\\nPérdida en el entrenamiento:', train_loss, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u7jo59Mmy0aJ"
   },
   "source": [
    "## Pruebas y predicciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewWImRKk_ko6",
    "outputId": "db5875a3-78ff-4467-fd54-5b16c6f33105"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 0.3324 - accuracy: 0.8931 - 775ms/epoch - 2ms/step\n",
      "\n",
      "Exactitud en la prueba: 0.8931000232696533 \n",
      "Pérdida en la prueba: 0.33242884278297424\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x_test,  y_test_onehot, verbose=2)\n",
    "\n",
    "print('\\nExactitud en la prueba:', test_acc, '\\nPérdida en la prueba:', test_loss, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InnitWkx-MxI"
   },
   "source": [
    "Finalmente se compara el modelo con los datos que no fueron utilizados para el entrenamiento, con la finalidad de valorar qué tan asertivo es el algoritmo de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZqrDzHOy0Jv"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmAcfbKx-eCc"
   },
   "source": [
    "La siguiente y última celda permite hacer pruebas con algún dato de posición \"validación\" y desplegar la comparativa entre el algoritmo programado y su valor real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GSBj__ltzAv9",
    "outputId": "9ec94f2b-bf33-4745-828f-942fb1b63bcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El predictor arroja una clase de tipo: Bag , la clase real es: Bag .\n"
     ]
    }
   ],
   "source": [
    "validacion = 1313\n",
    "predictions[validacion]\n",
    "res_predictor = np.argmax(predictions[validacion])\n",
    "res_real = y_test[validacion]\n",
    "print('El predictor arroja una clase de tipo:', label_dict[res_predictor], ', la clase real es:', label_dict[int(res_real)], '.')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
